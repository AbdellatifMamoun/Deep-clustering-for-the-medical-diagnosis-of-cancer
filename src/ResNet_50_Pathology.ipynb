{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ResNet-50 Pathology.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OyOQWMSKQYmG"
      },
      "source": [
        "# Introduction\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q-yv_Ii-4nKV"
      },
      "source": [
        "This project is included in the field of unsupervised learning, in particular deep clustering, which aims to properly classify unannotated data using classic clustering algorithms, such as kmeans and Birch, and use transfer learning to optimize work.\n",
        "In this composition, we show in stages the exploration of the data, extracting features, classifying these features and fine tuning our model which is based on the pre-trained **ResNet-50** network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sdRJ9YfW4LdT"
      },
      "source": [
        "# Manipulating the Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jSdlYWVMQhEf"
      },
      "source": [
        "In this part, we are interested in discovering our public dataset **colorectal_histology**, that classifies textures in colorectal cancer histology. Each example is a 150 x 150 x 3 RGB image of one of 8 classes.\n",
        "\n",
        "In this context, we will see our dataset informations, and visualize some  samples."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CY0SQkXP0xFI"
      },
      "source": [
        "The dataset can be changeable by modifying the next cell, where you can build or import a new data. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tZRSiDYNQLSr"
      },
      "source": [
        "# import and load \"colorectal_histology\" Dataset :\n",
        "import tensorflow_datasets as tfds\n",
        "ds, info = tfds.load('colorectal_histology', data_dir = '/input', split='train', shuffle_files=True, with_info=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ez9-GPcjlcTN"
      },
      "source": [
        "# Get dataset informations\n",
        "print(info)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YOckP_sYQcda"
      },
      "source": [
        "# Show samples of dataset -- discovering\n",
        "examples = tfds.show_examples(ds, info, rows=5, cols=5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-cE8hxtplzb"
      },
      "source": [
        "# Functions, librairies and routines"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ira3GEdVpuub"
      },
      "source": [
        "Here in this section, we will present several packages, modules and functions used to implement our Deep Cluster."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "87T5BetZQYQv"
      },
      "source": [
        "# Import necessary modules and packages:\n",
        "## import Architecture \"ResNet-50\"\n",
        "from tensorflow.keras.applications.resnet50 import ResNet50\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.applications.resnet50 import preprocess_input, decode_predictions\n",
        "import numpy as np\n",
        "##\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "##\n",
        "import keras\n",
        "from keras.models import Model\n",
        "##\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator \n",
        "from tensorflow.keras import optimizers\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, InputLayer\n",
        "from keras.models import Sequential\n",
        "#\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics.cluster import normalized_mutual_info_score\n",
        "#\n",
        "from sklearn.manifold import TSNE\n",
        "import pandas as pd\n",
        "import seaborn as sn\n",
        "#\n",
        "from google.colab import files\n",
        "#\n",
        "from keras.optimizers import SGD\n",
        "#\n",
        "from keras import utils\n",
        "from tensorflow.keras.initializers import RandomNormal\n",
        "from keras.models import load_model\n",
        "from keras.utils.np_utils import to_categorical\n",
        "import random\n",
        "from keras.utils.np_utils import to_categorical"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nMvL3u8_phQW"
      },
      "source": [
        "def clustering(features, clusters, variance = 0.98, alg = \"kmeans\"):\n",
        "  '''\n",
        "  Clustering features to a specific number (int) using one of the two \n",
        "  clustering algorithms : kmeans or birch.\n",
        "  For the speed and efficiency of clustering, the data must go through \n",
        "  the PCA step which allows to reduce the main components \n",
        "  according to a specific variance (float < 1).\n",
        "\n",
        "  Parameters:\n",
        "    features (array): array containing features\n",
        "    clusters (int): number of clusters to create\n",
        "    variance(float): variance of pca\n",
        "    alg (str): two choices -kmeans excuting kmeans alg; else execute birch alg.\n",
        "\n",
        "  Returns:\n",
        "    new_labels (array): list of predicted labels after clustering.\n",
        "  '''\n",
        "  cl = StandardScaler().fit_transform(features)\n",
        "  variance = 0.98\n",
        "  pca = PCA(variance)\n",
        "  pca.fit(cl) \n",
        "  cl = pca.transform(cl)\n",
        "  if alg ==\"kmeans\" :\n",
        "    kmeans = KMeans(init = \"k-means++\", n_clusters = clusters, n_init =25)\n",
        "    kmeans.fit(cl)\n",
        "    new_labels = kmeans.labels_ \n",
        "  else :\n",
        "    brc = Birch(n_clusters=clusters).fit(cl)\n",
        "    new_labels= brc.labels_\n",
        "  return new_labels\n",
        "\n",
        "\n",
        "def TSNE_plot(X,labels,ckpt, save=True):\n",
        "  '''\n",
        "  TSNE plot to visualize clusters according to labels associated to features.\n",
        "  This function allows to plot and save a caption of the results\n",
        "\n",
        "  Parameters:\n",
        "    X (array): array containing features\n",
        "    labels (array): array containing labels\n",
        "    ckpt (int): image title used to save a caption\n",
        "    save (Bool): choose to save the caption or not\n",
        "\n",
        "  Returns:\n",
        "    None\n",
        "    -> Visualize figure of clusters\n",
        "  '''\n",
        "  X=StandardScaler().fit_transform(X)\n",
        "  tsne_data = TSNE(n_components=2,random_state=0, perplexity=50.0).fit_transform(X)\n",
        "  tsne_data = np.vstack((tsne_data.T,labels)).T\n",
        "  tsne_df = pd.DataFrame(data=tsne_data, columns=(\"Dim_1\", \"Dim_2\", \"label\"))\n",
        "  sn.FacetGrid(tsne_df, hue=\"label\", size = 6).map(plt.scatter,'Dim_1', 'Dim_2' ).add_legend()\n",
        "  plt.savefig(\"%d.png\" %ckpt)\n",
        "  if save:\n",
        "    files.download(\"%d.png\" %ckpt)\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "def most_frequent(List): \n",
        "  '''\n",
        "  give the most frequent element of a list.\n",
        "\n",
        "  Parameters:\n",
        "    List (array): array, object of study\n",
        "\n",
        "  Returns:\n",
        "    num (type of the elements in List): most frequent element.\n",
        "  '''\n",
        "  counter = 0\n",
        "  num = List[0] \n",
        "      \n",
        "  for i in List: \n",
        "      curr_frequency = List.count(i) \n",
        "      if(curr_frequency> counter): \n",
        "        counter = curr_frequency \n",
        "        num = i \n",
        "  \n",
        "  return num \n",
        "\n",
        "\n",
        "def samples(id_cluster, klabels, labels, images, pp)\n",
        "  '''\n",
        "  Visualize some samples of a predicted class and show their original cluster.\n",
        "\n",
        "  Parameters:\n",
        "    id_cluster (int): array, object of study\n",
        "    klabels (array): predicted labels\n",
        "    labels (array): true labels\n",
        "    pp (int) : number of samples\n",
        "    images : images of dataset\n",
        "  Returns:\n",
        "    None\n",
        "    -> show samples\n",
        "  '''\n",
        "  cluster_members = (labels == id_cluster)\n",
        "  fig = plt.figure(figsize=(10,10));\n",
        "  m=0\n",
        "  p=1\n",
        "  while True : \n",
        "    if cluster_members[m]:\n",
        "      img= images[m] \n",
        "      label = list(labels[m]).index(1)\n",
        "      fig.add_subplot(5,5,p) \n",
        "      plt.imshow(img)\n",
        "      plt.title(str(label))\n",
        "      p+=1\n",
        "    m+=1\n",
        "    if p > pp*pp:\n",
        "      break\n",
        "  plt.show()\n",
        "\n",
        "def map(a,b, real_labels, dict, NUM_CLUSTER):\n",
        "  '''\n",
        "  from a given predicted and real labels, we create an equivalent array containing matches of real labels\n",
        "  (transform predicted labels to real labels == bijective application)\n",
        "\n",
        "  Parameters:\n",
        "    a (int): start index\n",
        "    b (int): end index\n",
        "    real_labels (array): array containing labels\n",
        "    dict (str): two choices -kmeans excuting kmeans alg; else execute birch alg.\n",
        "    NUM_CLUSTER (int):\n",
        "  Returns:\n",
        "    map (list): list of predicted labels after clustering.\n",
        "  '''\n",
        "  map =[]\n",
        "  for elm in real_labels[a:b]:\n",
        "    for i in dict.keys() : \n",
        "      if dict[i] == elm : \n",
        "        map.append(i)\n",
        "  return map\n",
        "  \n",
        "\n",
        "def dict_init(real_labels, new_labels, NUM_CLUSTER):\n",
        "  '''\n",
        "  Search the equivalents of the predicted labels with respect to the real labels.\n",
        "\n",
        "  Parameters:\n",
        "    real_labels (array): array containing original labels\n",
        "    new_labels (int): array containing predicted labels\n",
        "    NUM_CLUSTER (int): number of clusters\n",
        "\n",
        "  Returns:\n",
        "    dict (dict): dict object contains label matches.\n",
        "  '''\n",
        "  dict = {}\n",
        "  used = []\n",
        "  for i in range(NUM_CLUSTER) :\n",
        "    members = (new_labels == i)\n",
        "    clus = [ real_labels[i] for i in range(len(new_labels)) if members [i]]\n",
        "    ks = dom(clus)\n",
        "    j = 0\n",
        "    while ks[j] in used:\n",
        "      if j < len(ks) -1 :\n",
        "        j+=1\n",
        "      else :\n",
        "        l = [i for i in range(NUM_CLUSTER) if not(i in used)]\n",
        "        val= list(dict.values()).index(ks[j])\n",
        "        dict[val] = l[0]\n",
        "        used.append(l[0])\n",
        "        used.remove(ks[j])\n",
        "    dict[i] = ks[j]\n",
        "    used.append(ks[j])\n",
        "  return dict  \n",
        "\n",
        "\n",
        "def dom(liste): \n",
        "  '''\n",
        "  List the elements of the list according to their decreasing order of appearance.\n",
        "\n",
        "  Parameters:\n",
        "    liste (list): list, object of study\n",
        "\n",
        "  Returns:\n",
        "    res (list): list sorted by order of appearance.\n",
        "  '''\n",
        "  res = []\n",
        "  l = liste\n",
        "  while len(l)>=1 :\n",
        "    m = most_frequent(l)\n",
        "    res.append(m)\n",
        "    l = list(filter(lambda x: x != m, l))\n",
        "  return res"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WCYaA1TbRMTc"
      },
      "source": [
        "# Image Pre-processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cYPCHqRWpU0Z"
      },
      "source": [
        "At this point, we need to generate tensors. This task can be done by ImageDataGenerator, that generates batches of tensor image data with real-time pre-processing. we need to mention that we send the original images to the model, we just scale the image pixels between 0 and 1 and do not apply any transformations.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_F9FfyFSQl_5"
      },
      "source": [
        "# absolute path for our dataset : \n",
        "data_path = '/input/downloads/extracted/ZIP.zeno.org_reco_5316_file_Kath_text_2016_imaqL7TPMR0wf27knUqk31h7Z3Aye3ukvUAeDFu7zhZbcQ.zip/Kather_texture_2016_image_tiles_5000'\n",
        "\n",
        "# Rescaling -- Data augmentation :\n",
        "# We need scale the image pixels between 0 and 1.\n",
        "train_datagen = ImageDataGenerator(rescale=1./255)\n",
        "# And then, we generate batches of tensor image.\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        data_path,\n",
        "        target_size=(150, 150),\n",
        "        batch_size= 20,\n",
        "        class_mode= 'categorical')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0L8jc6uQ3_3A"
      },
      "source": [
        "# Deep Cluster : **Iteration 0**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HneCh6qxlXqR"
      },
      "source": [
        "In this section, we will implement the first iteration, named \"iteration 0\". This iteration is a set up to the main program that build our deep cluster model.\n",
        "\n",
        "The first iteration is about creating a model is based the pre-trained neural network **ResNet-50**, using random weights. \n",
        "Learning models will be used in the next section, where we will improve our neural network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xalsFDNyQp66"
      },
      "source": [
        "# pre trained model \"ResNet-50\"\n",
        "resnet = ResNet50(include_top=False, input_shape=(150,150,3), weights='imagenet', pooling='avg')\n",
        "resnet.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XryjXIYhQ_gM"
      },
      "source": [
        "# Extracting features with RESNET-50  pre-trained model :\n",
        "\n",
        "sample_count = 5000\n",
        "batch_size = 20\n",
        "features = np.zeros(shape=(sample_count, 2048))\n",
        "images = np.zeros(shape=(sample_count, 150, 150, 3))\n",
        "labels = np.zeros(shape=(sample_count, 8))\n",
        "i=0\n",
        "for inputs, lab in train_generator:\n",
        "    images[i*batch_size:(i+1)*batch_size] = inputs\n",
        "    labels[i*batch_size:(i+1)*batch_size] = lab\n",
        "    features[i*batch_size:(i+1)*batch_size]= resnet.predict(inputs)\n",
        "    i+=1\n",
        "    if i*batch_size >= sample_count : \n",
        "      break\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "beJ_3jq4JkDD"
      },
      "source": [
        "# Get predicted labels from new extracted features:\n",
        "new_labels = clustering(features, 8)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lD-L45PW-JFf"
      },
      "source": [
        "# Visualization : TSNE plot used to see predicted clusters\n",
        "TSNE_plot(features,new_labels, 0)\n",
        "# Note : results are random and not ordered as expected from iteration Â°0."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z_Hrqz3x-VEQ"
      },
      "source": [
        "# Adding the Dense Classifier of the Cluster Assignements\n",
        "\n",
        "x = resnet.output\n",
        "x = Flatten(name='flatten')(x)\n",
        "#x = Dense(128,activation='relu', kernel_initializer=RandomNormal(mean=0.0, stddev=0.001))(x)\n",
        "x = Dropout(0.5)(x)\n",
        "x = Dense(8, activation='softmax', name='fc8', kernel_initializer=RandomNormal(mean=0.0, stddev=0.001))(x)\n",
        "\n",
        "net = Model(inputs=resnet.input, outputs=x)\n",
        "\n",
        "net.trainable = False\n",
        "net.save('checkpoint/0.ckpt') "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lRadVwTi4GBv"
      },
      "source": [
        "# Deep Cluster : **Main algorithm**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1tLS9L-Vx9xE"
      },
      "source": [
        "This section is about fine tuning our neural network.\n",
        "One may build a dataset (images, Cluster assignments) that we need to fine-tune our model; where on top of which we add some dense layers to perform the task of classification. In addition, for fine-tuning the network, in order to establish a good behavior off fully-connected layers on the top, we wil Generate weights of the dense layer using the Kernel initializer of which we choose the distribution (Normal distribution with a low\n",
        "standard deviation and a mean equal to zero and training them both at the same time using the SGD optimizer with a low learning rate. \n",
        "\n",
        "The stop criterion of this algorithm is purely is exprimental\n",
        "and based on TSNE plot, confusion matrix and accuracies calculated from this matrix, normally in the order of 10 to 15 iterations.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LUTDOOFi-XmV"
      },
      "source": [
        "#Our Training/Validation Data\n",
        "train_images = images[1000:]\n",
        "train_labels = to_categorical(new_labels[1000:])\n",
        "test_images = images[:1000]\n",
        "\n",
        "# learning\n",
        "true_labels = np.argmax(labels, axis = -1)\n",
        "START = 1\n",
        "END = 15\n",
        "BATCH_SIZE = 20\n",
        "variance = 0.98\n",
        "NUM_CLUSTER = 8\n",
        "NUM_EPOCH = 20\n",
        "nmi_score = []\n",
        "confusion_score=[]\n",
        "\n",
        "for ckpt in range(START, END+1):\n",
        "  previous_labels = new_labels\n",
        "  previous_model = load_model('checkpoint/%d.ckpt'%(ckpt-1))\n",
        "  net = Model(inputs=previous_model.input, outputs=previous_model.get_layer('avg_pool').output)\n",
        "  \n",
        "  # extract features\n",
        "  new_features = []\n",
        "  for img in images:\n",
        "    img = image.img_to_array(img)\n",
        "    img = np.expand_dims(img, axis=0)\n",
        "    new_features.append(np.squeeze(net.predict(img),axis=0))\n",
        "  new_features = np.array(new_features)\n",
        "\n",
        "  # clustering\n",
        "  new_labels = clustering(new_features, 8)\n",
        "  train_labels = to_categorical(new_labels[1000:])\n",
        "  #Vizualising the clusters :\n",
        "  TSNE_plot(new_features,new_labels,ckpt) # to visualize predicted clusters \n",
        "  # TSNE_plot(new_features,true_labels,ckpt) # to visualize clusters according to their real labels\n",
        "  nmi_score.append(normalized_mutual_info_score(new_labels,previous_labels))\n",
        "  \n",
        "  # retrain: fine tune\n",
        "  init_model = load_model('checkpoint/0.ckpt')\n",
        "  x = init_model.get_layer('avg_pool').output\n",
        "  x = Flatten(name='flatten')(x)\n",
        "  x = Dense(128, activation='relu', kernel_initializer=RandomNormal(mean=0.0, stddev=0.001))(x)\n",
        "  x = Dropout(0.5)(x)\n",
        "  x = Dense(NUM_CLUSTER, activation='softmax', name='fc8', kernel_initializer=RandomNormal(mean=0.0, stddev=0.001))(x)\n",
        "  net = Model(inputs=init_model.input, outputs=x)\n",
        "  \n",
        "  for layer in net.layers:\n",
        "    layer.trainable = True\n",
        "\n",
        "  net.compile(loss='categorical_crossentropy',\n",
        "              optimizer=SGD(lr=0.001, momentum=0.9),\n",
        "               metrics=['acc'])\n",
        "  \n",
        "  net.fit(train_images, train_labels, batch_size = BATCH_SIZE,epochs = NUM_EPOCH)\n",
        "  \n",
        "  # Matrice de confusion\n",
        "  init_dicct = dict_init(true_labels, new_labels, NUM_CLUSTER)\n",
        "  cm=confusion_matrix(new_labels[:1000],map(0,1000,true_labels, init_dicct, NUM_CLUSTER))\n",
        "  sn.heatmap(data=cm,fmt='.0f',xticklabels=range(NUM_CLUSTER),yticklabels=range(NUM_CLUSTER),annot=True)\n",
        "\n",
        "  # score de performance\n",
        "  confusion_score.append(accuracy_score(new_labels[3000:],map(3000,5000,true_labels,init_dicct, NUM_CLUSTER)))\n",
        "  #print('Average precision-recall score: {0:0.2f}'.format(average_precision_score(new_labels[3000:],map(3000,5000,true_labels,init_dicct, NUM_CLUSTER)))\n",
        "  \n",
        "  #Save the model : \n",
        "\n",
        "  net.save('checkpoint/%d.ckpt'%ckpt)\n",
        "\n",
        "# Plot nmi :\n",
        "epochs = range(1, len(nmi_score) + 1)\n",
        "\n",
        "plt.plot(epochs, nmi_score, 'b', label='nmi score')\n",
        "plt.title('nmi score t/t-1')\n",
        "\n",
        "plt.legend() \n",
        "plt.show()\n",
        "\n",
        "plt.figure() \n",
        "plt.plot(epochs, confusion_score, 'b', label='Test loss')\n",
        "plt.title('Clustering accuracy score')\n",
        "plt.legend()\n",
        "\n",
        "plt.show() \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5lzMvSlj4gpy"
      },
      "source": [
        "# Conclusion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XDOOM3LY4fxM"
      },
      "source": [
        "In this project, we succeeded in constructing a deep cluster model, which allows to classify the images well as shown by the results in the section.\n",
        "\n",
        "The results were generally good and satisfactory. Our design can be improved by doing the similarity testing to select reliable training samples in order to avoid redundancy in our model.\n",
        "\n",
        "Generally, the deep clustering task was successfully as the model was able to adapt and recognize clusters of different type of images. Therfore, deep\n",
        "learning and domain adaptation are considerably the key to improve the field of pathology AI, especially, for pathology data that has not yet been\n",
        "processed.\n"
      ]
    }
  ]
}