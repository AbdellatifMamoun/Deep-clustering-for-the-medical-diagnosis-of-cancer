{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "S:VGG16 25GB RAM .ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yhG63voG4Hy8"
      },
      "source": [
        "# Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QuT-Nrjl4Oih"
      },
      "source": [
        "This project enters the field of unsupervised learning, in particular deep clustering, which aims to properly classify unannotated data using classic clustering algorithms, such as kmeans and Birch, and use transfer learning to optimize work. In this composition, we show in stages the exploration of the data, extracting features, classifying these features and fine tuning our model which is based on the pre-trained VGG-16 network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c_V-FXsh4aQp"
      },
      "source": [
        "# Exploring Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g6WlM8tA4jmC"
      },
      "source": [
        "In this part, we are interested in discovering our public dataset colorectal_histology, that classifies textures in colorectal cancer histology. Each example is a 150 x 150 x 3 RGB image of one of 8 classes.\n",
        "\n",
        "In this context, we will see our dataset informations, and visualize some samples.\n",
        "\n",
        "The dataset can be changeable by modifying the next cellule, where you can build or import a new data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "esYFByf-cHjz"
      },
      "source": [
        "# import and load \"colorectal_histology\" Dataset :\n",
        "import tensorflow_datasets as tfds\n",
        "ds, info = tfds.load('colorectal_histology', split='train',data_dir='/Users/asus/Desktop/Colorectal_histology', shuffle_files=True, with_info=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u8i9PxCH4sMq"
      },
      "source": [
        "print(info)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YOckP_sYQcda"
      },
      "source": [
        "# Show samples of dataset -- discovering\n",
        "examples = tfds.show_examples(ds, info, rows=5, cols=5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wAkRW_X94w_R"
      },
      "source": [
        "# Functions and librairies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ira3GEdVpuub"
      },
      "source": [
        "Here in this section, we will present several packages, modules and functions used to implement our Deep Cluster."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dkmc3pj0svUw"
      },
      "source": [
        "def TSNE_plot(X,labels,ckpt, save=True):\n",
        "  '''\n",
        "  TSNE plot to visualize clusters according to labels associated to features.\n",
        "  This function allows to plot and save a caption of the results\n",
        "\n",
        "  Parameters:\n",
        "    X (array): array containing features\n",
        "    labels (array): array containing labels\n",
        "    ckpt (int): image title used to save a caption\n",
        "    save (Bool): choose to save the caption or not\n",
        "\n",
        "  Returns:\n",
        "    None\n",
        "    -> Visualize figure of clusters\n",
        "  '''\n",
        "  X=StandardScaler().fit_transform(X)\n",
        "  tsne_data = TSNE(n_components=2,random_state=0, perplexity=50.0).fit_transform(X)\n",
        "  tsne_data = np.vstack((tsne_data.T,labels)).T\n",
        "  tsne_df = pd.DataFrame(data=tsne_data, columns=(\"Dim_1\", \"Dim_2\", \"label\"))\n",
        "  sn.FacetGrid(tsne_df, hue=\"label\", size = 6).map(plt.scatter,'Dim_1', 'Dim_2' ).add_legend()\n",
        "  plt.savefig(\"%d.png\" %ckpt)\n",
        "  if save:\n",
        "    files.download(\"%d.png\" %ckpt)\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "def most_frequent(List): \n",
        "  '''\n",
        "  give the most frequent element of a list.\n",
        "\n",
        "  Parameters:\n",
        "    List (array): array, object of study\n",
        "\n",
        "  Returns:\n",
        "    num (type of the elements in List): most frequent element.\n",
        "  '''\n",
        "  counter = 0\n",
        "  num = List[0] \n",
        "      \n",
        "  for i in List: \n",
        "      curr_frequency = List.count(i) \n",
        "      if(curr_frequency> counter): \n",
        "        counter = curr_frequency \n",
        "        num = i \n",
        "  \n",
        "  return num \n",
        "\n",
        "\n",
        "def map(a,b, real_labels, dict, NUM_CLUSTER):\n",
        "  '''\n",
        "  from a given predicted and real labels, we create an equivalent array containing matches of real labels\n",
        "  (transform predicted labels to real labels == bijective application)\n",
        "\n",
        "  Parameters:\n",
        "    a (int): start index\n",
        "    b (int): end index\n",
        "    real_labels (array): array containing labels\n",
        "    dict (str): two choices -kmeans excuting kmeans alg; else execute birch alg.\n",
        "    NUM_CLUSTER (int):\n",
        "  Returns:\n",
        "    map (list): list of predicted labels after clustering.\n",
        "  '''\n",
        "  map =[]\n",
        "  for elm in real_labels[a:b]:\n",
        "    for i in dict.keys() : \n",
        "      if dict[i] == elm : \n",
        "        map.append(i)\n",
        "  return map\n",
        "  \n",
        "  \n",
        "def dict_init(real_labels, new_labels, NUM_CLUSTER):\n",
        "  '''\n",
        "  Search the equivalents of the predicted labels with respect to the real labels.\n",
        "\n",
        "  Parameters:\n",
        "    real_labels (array): array containing original labels\n",
        "    new_labels (int): array containing predicted labels\n",
        "    NUM_CLUSTER (int): number of clusters\n",
        "\n",
        "  Returns:\n",
        "    dict (dict): dict object contains label matches.\n",
        "  '''\n",
        "  dict = {}\n",
        "  used = []\n",
        "  for i in range(NUM_CLUSTER) :\n",
        "    members = (new_labels == i)\n",
        "    clus = [ real_labels[i] for i in range(len(new_labels)) if members [i]]\n",
        "    ks = dom(clus)\n",
        "    j = 0\n",
        "    while ks[j] in used:\n",
        "      if j < len(ks) -1 :\n",
        "        j+=1\n",
        "      else :\n",
        "        l = [i for i in range(NUM_CLUSTER) if not(i in used)]\n",
        "        val= list(dict.values()).index(ks[j])\n",
        "        dict[val] = l[0]\n",
        "        used.append(l[0])\n",
        "        used.remove(ks[j])\n",
        "    dict[i] = ks[j]\n",
        "    used.append(ks[j])\n",
        "  return dict  \n",
        "\n",
        "\n",
        "def dom(liste): \n",
        "  '''\n",
        "  List the elements of the list according to their decreasing order of appearance.\n",
        "\n",
        "  Parameters:\n",
        "    liste (list): list, object of study\n",
        "\n",
        "  Returns:\n",
        "    res (list): list sorted by order of appearance.\n",
        "  '''\n",
        "  res = []\n",
        "  l = liste\n",
        "  while len(l)>=1 :\n",
        "    m = most_frequent(l)\n",
        "    res.append(m)\n",
        "    l = list(filter(lambda x: x != m, l))\n",
        "  return res\n",
        "\n",
        "def freeze_until(layr,net):\n",
        "  '''\n",
        "  train or freeze some layers sequentially.\n",
        "\n",
        "  Parameters:\n",
        "    layer (str): Name of layer\n",
        "    net (Object) : neural network\n",
        "  Returns:\n",
        "    None\n",
        "    -> Unfreezing selected layers\n",
        "  '''\n",
        "  net.trainable = True \n",
        "  set_trainable = False\n",
        "  for layer in net.layers:\n",
        "    if layer.name == layr:\n",
        "      set_trainable = True\n",
        "    if set_trainable:\n",
        "      layer.trainable = True\n",
        "    else:\n",
        "      layer.trainable = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "87T5BetZQYQv"
      },
      "source": [
        "#All the imports needed : \n",
        "from google.colab import files\n",
        "from keras.preprocessing import image\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator \n",
        "from tensorflow.keras import optimizers\n",
        "from tensorflow.keras import Model\n",
        "from keras.utils.np_utils import to_categorical\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics.cluster import normalized_mutual_info_score\n",
        "from keras import models\n",
        "from keras import layers\n",
        "from keras.layers import Dense, Flatten, Dropout\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sn\n",
        "import pandas as pd\n",
        "\n",
        "#Parameters :\n",
        "sample_count = 5000\n",
        "batch_size = 20"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FmSMfilC5IF1"
      },
      "source": [
        "# Image Pre-processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cYPCHqRWpU0Z"
      },
      "source": [
        "At this point, we need to generate tenor. This task can be done by ImageDataGenerator, that generates batches of tensor image data with real-time pre-processing. we need to mention that we send the original test images to the model, we just scale the image pixels between 0 and 1 and do not apply any transformations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6GwzsdHDgjT0",
        "outputId": "b9e86cf8-d8f2-4ad0-b9a2-7950a4500552"
      },
      "source": [
        "train_dir = '/Users/asus/Desktop/Colorectal_histology/downloads/extracted/ZIP.zeno.org_reco_5316_file_Kath_text_2016_imaqL7TPMR0wf27knUqk31h7Z3Aye3ukvUAeDFu7zhZbcQ.zip/Kather_texture_2016_image_tiles_5000'\n",
        "\n",
        "#Scaling Images \n",
        "\n",
        "train_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        train_dir,\n",
        "        target_size=(150, 150),\n",
        "        batch_size= 20,\n",
        "        class_mode= 'categorical')\n",
        "\n",
        "print('Rescaling done')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 5000 images belonging to 8 classes.\n",
            "Rescaling done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CCeMoDrI5Ygk"
      },
      "source": [
        "# Deep Cluster : Iteration 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HneCh6qxlXqR"
      },
      "source": [
        "In this section, we will implement the first iteration, named \"iteration 0\". This iteration is a set up to the main program that build our deep cluster model.\n",
        "\n",
        "The first iteration is about creating a model is based the pre-trained neural network **VGG-16**, using random weights. \n",
        "Learning models will be used in the next section, where we will improve our neural network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uFBgBOMpfqhU"
      },
      "source": [
        "# pre trained model \"VGG-16\"\n",
        "conv_base = VGG16(weights='imagenet',\n",
        "                  include_top= False,\n",
        "                  input_shape=(150,150,3),\n",
        "                  pooling='avg')\n",
        "conv_base.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RGFCvBfIfu2d"
      },
      "source": [
        "#Extracting features using the VGG16's convolutional base\n",
        "\n",
        "features = np.zeros(shape=(sample_count,512))\n",
        "labels = np.zeros(shape=(sample_count,8))\n",
        "images = np.zeros(shape=(sample_count,150,150,3))\n",
        "i=0\n",
        "for inputs_batch, labels_batch in train_generator:\n",
        "    images[i*batch_size:(i+1)*batch_size]= inputs_batch\n",
        "    labels[i*batch_size:(i+1)*batch_size]= labels_batch\n",
        "    feature_batch = conv_base.predict(inputs_batch)\n",
        "    features[i*batch_size:(i+1)*batch_size]= feature_batch\n",
        "    i+=1\n",
        "    if i*batch_size >= sample_count:\n",
        "        break\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T0cPdoxqrYGo"
      },
      "source": [
        "Clus_dataSet= features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t3rU7ZaV81NF"
      },
      "source": [
        "#n_init=24 is satisfactory according to the plot\n",
        "k_means = KMeans(init = \"k-means++\", n_clusters = 8, n_init =25 )\n",
        "k_means.fit( Clus_dataSet)\n",
        "k_means_labels = k_means.labels_ #List of labels of each dataset\n",
        "#labels\n",
        "new_labels=k_means_labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HupLecWkfNQQ"
      },
      "source": [
        "#Training the Dense Classifier of the Cluster Assignements\n",
        "#Our Training/Validation Data\n",
        "test_images = images[4000:]\n",
        "\n",
        "x = conv_base.output\n",
        "x = Dense(128,activation='relu')(x)\n",
        "#x = Dropout(0.5)(x)\n",
        "x = Dense(8,activation='softmax')(x)\n",
        "\n",
        "\n",
        "model = Model(inputs=conv_base.input, outputs=x)\n",
        "\n",
        "conv_base.trainable = False\n",
        "\n",
        "\n",
        "model.summary()\n",
        "model.save('checkpoint/0.ckpt') \n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GXIuQq8B6Cl9"
      },
      "source": [
        "# Deep Cluster"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xXFbOWjCGtbA"
      },
      "source": [
        "# Iterate : \n",
        "#   Extract feature\n",
        "#   Clustering k - means ==> labels\n",
        "#   Construct training data , (images,labels)\n",
        "#   Unfreeze lasts convolutionnal layers + concatenate dense layer\n",
        "#   Train using k-means assignements\n",
        "#   Use the new convolutinnal base to extract features\n",
        "from keras.models import load_model\n",
        "\n",
        "confusion_score=[]\n",
        "true_labels = np.argmax(labels,axis=-1)\n",
        "import sys\n",
        "import tensorflow as tf\n",
        "tf.compat.v1.disable_eager_execution()\n",
        "\n",
        "# \n",
        "center_t = tf.compat.v1.placeholder(tf.float32, (None, None))\n",
        "other_t = tf.compat.v1.placeholder(tf.float32, (None, None))\n",
        "center_t_norm = tf.compat.v1.linalg.l2_normalize(center_t, dim=1)\n",
        "other_t_norm = tf.compat.v1.linalg.l2_normalize(other_t, dim=1)\n",
        "similarity = tf.matmul(center_t_norm, other_t_norm, transpose_a=False, transpose_b=True)\n",
        "\n",
        "\n",
        "\n",
        "# learning\n",
        "START = 1\n",
        "END = 8\n",
        "batch_size = 20\n",
        "NUM_CLUSTER = 8\n",
        "nmi_score = []\n",
        "\n",
        "for ckpt in range(START, END+1):\n",
        "  previous_labels = new_labels\n",
        "  previous_model = load_model('checkpoint/%d.ckpt'%(ckpt-1))\n",
        "  net = Model(inputs=previous_model.input, outputs=previous_model.get_layer('global_average_pooling2d').output)\n",
        "  \n",
        "  # extract features\n",
        "  new_features = []\n",
        "  for img in images:\n",
        "    img = image.img_to_array(img)\n",
        "    img = np.expand_dims(img, axis=0)\n",
        "    new_features.append(np.squeeze(net.predict(img),axis=0))\n",
        "  new_features = np.array(new_features)\n",
        "\n",
        "  # clustering\n",
        "  #features = PCA(features.reshape(-1,features.shape[1]*features.shape[2]*features.shape[3]))\n",
        "  k_means = KMeans(init = \"k-means++\", n_clusters = NUM_CLUSTER, n_init =25).fit(new_features)\n",
        "\n",
        "   # new_labels : \n",
        "  new_labels = k_means.labels_\n",
        "  true_labels = np.argmax(labels,axis=-1)\n",
        "\n",
        "  #Vizualising the clusters :\n",
        "  #TSNE_plot(new_features,new_labels,ckpt) # to visualize predicted clusters \n",
        "  TSNE_plot(new_features,true_labels,ckpt) # to visualize clusters according to their real labels\n",
        "  nmi_score.append(normalized_mutual_info_score(new_labels,previous_labels))\n",
        "  #nmi_score_2.append(normalized_mutual_info_score(new_labels,true_labels))\n",
        "\n",
        "  # select \n",
        "  tmp = new_features\n",
        "  distances = k_means.transform(tmp) # num images * NUM_CLUSTER\n",
        "  center_idx = np.argmin(distances, axis=0)\n",
        "  centers = [tmp[i] for i in center_idx]\n",
        "  \n",
        "  #calculate similarity matrix\n",
        "  with tf.compat.v1.Session() as sess:\n",
        "    similarities = sess.run(similarity, {center_t: centers, other_t: tmp[:4000]}) # NUM_CLUSTER * num images\n",
        "\n",
        "  # select reliable images\n",
        "  reliable_image_idx = np.unique(np.argwhere(similarities > 0.85)[:,1])\n",
        "  print ('ckpt %d: # reliable images %d'%(ckpt, len(reliable_image_idx)))\n",
        "  sys.stdout.flush()\n",
        "  train_images = np.array([images[i] for i in reliable_image_idx])\n",
        "  train_labels = to_categorical([new_labels[i] for i in reliable_image_idx])\n",
        "\n",
        "  #Building the training labels :\n",
        "  #train_labels = to_categorical(new_labels[:4000])\n",
        "\n",
        "  # retrain: fine tune\n",
        "  init_model = load_model('checkpoint/%d.ckpt'%(ckpt-1))\n",
        "  x = init_model.get_layer('global_average_pooling2d').output\n",
        "  #x = Dense(128, activation='relu')(x)\n",
        "  x = Dropout(0.5)(x)\n",
        "  x = Dense(NUM_CLUSTER, activation='softmax')(x)\n",
        "  net = Model(inputs=init_model.input, outputs=x)\n",
        " \n",
        " \n",
        "  #training the dense classifier first:\n",
        "  freeze_until('global_average_pooling2d',net)\n",
        "  net.compile(loss='categorical_crossentropy',\n",
        "              optimizer=optimizers.RMSprop(lr=2e-5),\n",
        "               metrics=['acc'])\n",
        " \n",
        "  history=net.fit(train_images,train_labels, batch_size=batch_size,epochs=15) \n",
        "  #Plots :\n",
        "\n",
        "\n",
        "  #Jointly train the last conv block and the classifier with smaller learning rate :\n",
        "  freeze_until('block5_conv1',net)\n",
        "  net.compile(loss='categorical_crossentropy',\n",
        "              optimizer=optimizers.RMSprop(lr=1e-5),\n",
        "               metrics=['acc'])\n",
        " \n",
        "  history=net.fit(train_images,train_labels, batch_size=batch_size,epochs=20)\n",
        "\n",
        "  ##Evaluating the neural network : \n",
        "  #test_labels = map(3000,5000,new_labels,true_labels, NUM_CLUSTER)\n",
        "  #eval = model.evaluate(test_data,test_labels, batch_size=batch_size)\n",
        "  #loss.append(eval[0])\n",
        "  #acc.append(eval[1])\n",
        "\n",
        "  init_dicct = dict_init(true_labels, new_labels, NUM_CLUSTER)\n",
        "  cm=confusion_matrix(new_labels[4000:],map(4000,5000,true_labels, init_dicct, NUM_CLUSTER))\n",
        "  sn.heatmap(data=cm,fmt='.0f',xticklabels=range(NUM_CLUSTER),yticklabels=range(NUM_CLUSTER),annot=True)\n",
        "\n",
        "  #Performance score:\n",
        "  confusion_score.append(accuracy_score(new_labels[4000:],map(4000,5000,true_labels,init_dicct, NUM_CLUSTER)))\n",
        " \n",
        "  #Save the model : \n",
        "\n",
        "  net.save('checkpoint/%d.ckpt'%ckpt)\n",
        "\n",
        "# Plot nmi , accuracy, loss\n",
        "plt.figure()\n",
        "epochs = range(1, len(nmi_score) + 1)\n",
        "\n",
        "plt.plot(epochs, nmi_score, 'b', label='nmi score')\n",
        "plt.title('nmi score t/t-1')\n",
        "\n",
        "plt.legend() \n",
        "\n",
        "\n",
        "\n",
        "plt.figure() \n",
        "plt.plot(epochs, confusion_score, 'b', label='accuracy')\n",
        "plt.title('Accuracy score')\n",
        "plt.legend()\n",
        "\n",
        "plt.show() \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1BVwINWf7gj1"
      },
      "source": [
        "# Conclusion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_AfQ-367kUj"
      },
      "source": [
        "In this project, we succeeded in constructing a deep cluster model, which allows to classify the images well as shown by the results in the section.\n",
        "\n",
        "The results were generally good and satisfactory. Our design can be improved by doing the similarity testing to select reliable training samples in order to avoid redundancy in our model.\n",
        "\n",
        "Generally, the deep clustering task was successfully as the model was able to adapt and recognize clusters of different type of images. Therfore, deep learning and domain adaptation are considerably the key to improve the field of pathology AI, especially, for pathology data that has not yet been processed."
      ]
    }
  ]
}